{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gY-93kdQYMLV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlpQ5a-cY2QO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/deepdaiv위급상황탐지/EfficientAT')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVU1YvMZBcbO"
      },
      "outputs": [],
      "source": [
        "demo_path1 = '/content/drive/MyDrive/deepdaiv위급상황탐지/EfficientAT/데모/소리 녹음'\n",
        "demo_list1 = os.listdir(demo_path1)\n",
        "demo_list1.remove('.ipynb_checkpoints')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k92nhWWoDxRL"
      },
      "outputs": [],
      "source": [
        "print(demo_list1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8FxEjBGGK7V"
      },
      "outputs": [],
      "source": [
        "demo_path2 = '/content/drive/MyDrive/deepdaiv위급상황탐지/EfficientAT/데모/Ai Hub 위급상황'\n",
        "demo_list2 = os.listdir(demo_path2)\n",
        "demo_list2.remove('.ipynb_checkpoints')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LaaLBNZGSCS"
      },
      "outputs": [],
      "source": [
        "print(demo_list2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gm7hFFhkUb5_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio\n",
        "from scipy.io import wavfile\n",
        "import librosa\n",
        "\n",
        "def screamdetect(audio_path,threshold,screamdetecting):\n",
        "  yamnet = hub.load('https://tfhub.dev/google/yamnet/1')\n",
        "\n",
        "  # Find the name of the class with the top score when mean-aggregated across frames.\n",
        "  def class_names_from_csv(class_map_csv_text):\n",
        "    \"\"\"Returns list of class names corresponding to score vector.\"\"\"\n",
        "    class_names = []\n",
        "    with tf.io.gfile.GFile(class_map_csv_text) as csvfile:\n",
        "      reader = csv.DictReader(csvfile)\n",
        "      for row in reader:\n",
        "        class_names.append(row['display_name'])\n",
        "\n",
        "    return class_names\n",
        "\n",
        "  def split_wav_data(wav_data, sample_rate, segment_length=1):\n",
        "    \"\"\"Split the wav data into segments of specified length.\"\"\"\n",
        "    num_samples_per_segment = sample_rate * segment_length\n",
        "    num_segments = int(np.ceil(len(wav_data) / num_samples_per_segment))\n",
        "    segments = []\n",
        "\n",
        "    for i in range(num_segments):\n",
        "        start = i * num_samples_per_segment\n",
        "        end = start + num_samples_per_segment\n",
        "        segment = wav_data[start:end]\n",
        "        segments.append(segment)\n",
        "\n",
        "    return segments\n",
        "\n",
        "\n",
        "  class_map_path = yamnet.class_map_path().numpy()\n",
        "  class_names = class_names_from_csv(class_map_path)\n",
        "  waveform , _ = librosa.core.load(audio_path,sr=16000,mono=True)\n",
        "  waveform = waveform.astype('float16')\n",
        "  segments = split_wav_data(waveform, 16000)\n",
        "  print('segments:',len(segments))\n",
        "\n",
        "  threshold = threshold\n",
        "  scream_idx = []\n",
        "  for i,wave in enumerate(segments):\n",
        "      scores, embeddings, spectrogram = yamnet(waveform)\n",
        "\n",
        "    # 결과를 numpy 배열로 변환\n",
        "      scores_np = scores.numpy()\n",
        "      spectrogram_np = spectrogram.numpy()\n",
        "\n",
        "      # 임계값을 초과하는 모든 클래스 식별\n",
        "      detected_classes = [class_names[idx] for idx, score in enumerate(scores_np.mean(axis=0)) if score > threshold]\n",
        "      if 'Screaming' in detected_classes:\n",
        "        scream_idx.append((i))\n",
        "        print(f'Segment {i}: Scream Detected')\n",
        "\n",
        "  if screamdetecting == True:\n",
        "    if len(scream_idx) == 0:\n",
        "      return False #비명이 감지되지않으면 일반상황판단\n",
        "\n",
        "    elif len(scream_idx) == len(segments):\n",
        "      scream_point = 0 #비명이 모든 오디오에서 감지되면 모든 오디오 위급상황 판단\n",
        "\n",
        "    else:\n",
        "      for i in range(len(scream_idx)-1):\n",
        "        if scream_idx[i+1] - scream_idx[i] != 1:\n",
        "          scream_point = scream_idx[i] #첫번째 비명이후부터 위급상황판단\n",
        "          break\n",
        "  else:\n",
        "    scream_point = 0\n",
        "\n",
        "  sampling_rate = 32000\n",
        "  waveform2 , _ = librosa.core.load(audio_path,sr=sampling_rate\n",
        "                                 ,mono=True)\n",
        "\n",
        "  segments2 = split_wav_data(waveform2,sampling_rate)\n",
        "\n",
        "  #비명이후 위급상황감지 슬라이딩 윈도우\n",
        "\n",
        "  clf_leng = len(segments2)-1 - scream_point\n",
        "  print('clf_leng',clf_leng)\n",
        "  #print(clf_leng)\n",
        "  wave_list = []\n",
        "  if clf_leng == 0:\n",
        "    if len(segments2) > 8:\n",
        "      additional = len(segments2) - 8\n",
        "      scream_point = 0\n",
        "      for i in list(range(additional+1)):\n",
        "        wave = np.concatenate(segments2[scream_point+i:scream_point+8+i])\n",
        "        wave_list.append(wave)\n",
        "        #print(len(wave_list))\n",
        "    else:\n",
        "        wave = np.concatenate(segments2)\n",
        "        wave_list.append(wave)\n",
        "\n",
        "  elif clf_leng > 8:\n",
        "    additional = clf_leng - 8\n",
        "    for i in list(range(additional+1)):\n",
        "      wave = np.concatenate(segments2[scream_point+i:scream_point+8+i])\n",
        "      wave_list.append(wave)\n",
        "  else:\n",
        "    wave = np.concatenate(segments2[scream_point+1:])\n",
        "    wave_list.append(wave)\n",
        "\n",
        "  return wave_list\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBIgVzxeZIHg"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "from torch import autocast\n",
        "from contextlib import nullcontext\n",
        "\n",
        "from models.mn.model_nonprint import get_model as get_mobilenet\n",
        "from models.dymn.model import get_model as get_dymn\n",
        "from models.ensemble import get_ensemble_model\n",
        "from models.preprocess import AugmentMelSTFT\n",
        "from helpers.utils import NAME_TO_WIDTH, labels\n",
        "\n",
        "\n",
        "def audio_tagging(args,wave_list):\n",
        "    if wave_list == False:\n",
        "      return print('현재상황:정상')\n",
        "\n",
        "    print('num of wave',len(wave_list))\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Running Inference on an audio clip.\n",
        "    \"\"\"\n",
        "    model_name = args.model_name\n",
        "    device = torch.device('cuda') if args.cuda and torch.cuda.is_available() else torch.device('cpu')\n",
        "    audio_path = args.audio_path\n",
        "    sample_rate = args.sample_rate\n",
        "    window_size = args.window_size\n",
        "    hop_size = args.hop_size\n",
        "    n_mels = args.n_mels\n",
        "    model_path = args.model_path\n",
        "    labels = ['강제추행(성범죄)', '강도범죄', '절도범죄', '폭력범죄',\n",
        "              '화재', '갇힘', '응급의료',\n",
        "              '전기사고', '가스사고', '낙상', '붕괴사고', '도움요청']\n",
        "\n",
        "    # load pre-trained model\n",
        "    '''if len(args.ensemble) > 0:\n",
        "        model = get_ensemble_model(args.ensemble)\n",
        "    else:'''\n",
        "    if model_name.startswith(\"dymn\"):\n",
        "        model = get_dymn(width_mult=NAME_TO_WIDTH(model_name), pretrained_name=model_name,\n",
        "                                  strides=args.strides,num_classes=12)\n",
        "    else:\n",
        "        #print('check1')\n",
        "        model = get_mobilenet(width_mult=NAME_TO_WIDTH(model_name), pretrained_name=model_name,\n",
        "                                  strides=args.strides, head_type=args.head_type,se_dims=args.se_dims,\n",
        "                                  num_classes=12)\n",
        "    model.to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    # model to preprocess waveform into mel spectrograms\n",
        "    mel = AugmentMelSTFT(n_mels=n_mels, sr=sample_rate, win_length=window_size, hopsize=hop_size)\n",
        "    mel.to(device)\n",
        "    mel.eval()\n",
        "\n",
        "    for i in range(len(wave_list)):\n",
        "      waveform = wave_list[i]\n",
        "      waveform = waveform.astype(np.float32)\n",
        "      #(waveform, _) = librosa.core.load(audio_path, sr=sample_rate, mono=True)\n",
        "      waveform = torch.from_numpy(waveform[None, :]).to(device)\n",
        "\n",
        "      # our models are trained in half precision mode (torch.float16)\n",
        "      # run on cuda with torch.float16 to get the best performance\n",
        "      # running on cpu with torch.float32 gives similar performance, using torch.bfloat16 is worse\n",
        "      with torch.no_grad(), autocast(device_type=device.type) if args.cuda else nullcontext():\n",
        "          spec = mel(waveform)\n",
        "          preds, features = model(spec.unsqueeze(0))\n",
        "      if i == 0:\n",
        "        predsum = preds\n",
        "\n",
        "        pred_idx = torch.argmax(preds,dim = 1)\n",
        "        situation = labels[pred_idx]\n",
        "        print(\"************* Detecting *****************\")\n",
        "        print(f'Window {i+1} 상황:{situation}')\n",
        "      else:\n",
        "        predsum += preds\n",
        "\n",
        "        pred_idx = torch.argmax(preds,dim = 1)\n",
        "        situation = labels[pred_idx]\n",
        "        print(\"************* Detecting *****************\")\n",
        "        print(f'Window {i+1} 상황:{situation}')\n",
        "\n",
        "    pred_idx = torch.argmax(predsum,dim = 1)\n",
        "    situation = labels[pred_idx]\n",
        "    print(\"************* Result *****************\")\n",
        "    print(f'현재 상황:{situation}')\n",
        "\n",
        "    #preds = torch.sigmoid(preds.float()).squeeze().cpu().numpy()\n",
        "    #sorted_indexes = np.argsort(preds)[::-1]\n",
        "\n",
        "    '''# Print audio tagging top probabilities\n",
        "    print(\"************* Acoustic Event Detected: *****************\")\n",
        "    for k in range(10):\n",
        "        print('{}: {:.3f}'.format(labels[sorted_indexes[k]],\n",
        "            preds[sorted_indexes[k]]))\n",
        "    print(\"********************************************************\")'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzG0YSz6oNtP"
      },
      "outputs": [],
      "source": [
        "def emergencydetect(args):\n",
        "  wave_list = screamdetect(args.audio_path,threshold=args.threshold,screamdetecting=args.screamdetecting)\n",
        "  audio_tagging(args,wave_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoKcVZdOFt91"
      },
      "source": [
        "#데모"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7IZDtNs-r0V"
      },
      "outputs": [],
      "source": [
        "!pip install -q IPython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Df1AQrdEEHj"
      },
      "outputs": [],
      "source": [
        "#영화 오디오\n",
        "import random\n",
        "idx = random.randint(0,len(demo_list1))\n",
        "#random_demo = demo_list1[idx]\n",
        "#['붕괴.wav', '절도 - 도깨비.wav', '폭행 - 수리남.wav', '화재 - 뉴스.wav', '강도 - 아는 와이프.wav']\n",
        "random_demo = demo_list1[3]\n",
        "audio_path = demo_path1 + '/' + random_demo\n",
        "print('Selected demo:',idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XJ5qrP6G6Ky"
      },
      "outputs": [],
      "source": [
        "#Ai-Hub 위급상황데이터\n",
        "import random\n",
        "idx = random.randint(0,len(demo_list2))\n",
        "random_demo = demo_list2[idx]\n",
        "audio_path = demo_path2 + '/' + random_demo\n",
        "print('Selected demo:',idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qyGt1axO-wWh"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "Audio(audio_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daBraMGgfApY"
      },
      "outputs": [],
      "source": [
        "import easydict\n",
        "\n",
        "args = easydict.EasyDict({'model_name':'mn10_as',\n",
        "                          'strides': [2,2,2,2],\n",
        "                          'head_type':'mlp',\n",
        "                          'se_dims':\"c\",\n",
        "                          'sample_rate':32000,\n",
        "                          'window_size':800,\n",
        "                          'hop_size':320,\n",
        "                          'n_mels':128,\n",
        "                          'cuda':True,\n",
        "                          'ensemble':[],\n",
        "                          'screamdetecting': False,\n",
        "                          'threshold': 0.0001,\n",
        "                          'audio_path': audio_path,\n",
        "                          'model_path':'/content/drive/MyDrive/deepdaiv위급상황탐지/EfficientAT/inference_test/mn10_custom_12class_epoch_14_mAP_100.pt'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbaQ-Tx2oxHR"
      },
      "outputs": [],
      "source": [
        "emergencydetect(args)\n",
        "#print(random_demo)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}